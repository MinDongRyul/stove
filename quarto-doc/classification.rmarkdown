---
title: "goophi"
date: 2022.08.22.
format:
  pdf:
    toc: true
    number-sections: true
    number-depth: 2
    colorlinks: true
    highlight-style: github-dark
mainfont: Gulim
---

```{r setup, message = FALSE}
library(goophi)
```


# Introduction

1) 본 문서는 goophi 패키지를 Shiny app에서 사용하는 것을 상정해 작성했습니다.

2) 본 문서의 케이스 스타일은 Camel case와 Snake case가 혼용되어 있습니다.
- Camel case : goophi의 함수명 및 파라미터명
- Snake case: 유저로부터 받는 입력, Shiny app의 server에서 사용(될 것이라고 예상)하는 Object명, snake case로 작성된 dependencies의 함수명 등

# Import sample data

1) 전처리가 완료된 샘플데이터를 불러옵니다. 
- NA가 없어야 함
- string value가 있는 열은 factor로 변환
- 한 열이 모두 같은 값으로 채워져 있을 경우 제외해야 함
- Date type column이 없어야 함
- Outcome 변수는 classification의 경우 factor, regression의 경우 numeric이어야 함 (clustering은 outcome변수를 사용하지 않음)


```{r}
cleaned_data <- read.csv(file = "~/git/goophi/data/boston_c.csv", 
                         stringsAsFactors = TRUE
                         )
cleaned_data$Pcrime <- as.factor(cleaned_data$Pcrime)
str(cleaned_data)
```


# Data Setup Tab

|    User Input   |                                       description |
|:---------------:|:-------------------------------------------------:|
|     target_var  |                                         목적 변수 |
| train_set_ratio |전체 데이터 중 train set의 비율 (range: 0.0 - 1.0) |

1) User input을 다음과 같이 받습니다.
- formula는 user가 target_var를 입력할 때 함께 생성되도록 함


```{r}
target_var <- "Pcrime"
train_set_ratio <- "0.7"
seed <- "1234"
formula <- paste0(target_var, " ~ .")
```


2) Train-test split 작업이 완료된 Object를 저장하고, Train set을 보여줍니다.


```{r}
split_tmp <- goophi::trainTestSplit(data = cleaned_data,
                                    target = target_var,
                                    prop = train_set_ratio,
                                    seed = seed
                                    )

data_train <- split_tmp[[1]] # train data
data_test <- split_tmp[[2]] # test data
data_split <- split_tmp[[3]] # whole data with split information
```


3) train set에 적용할 전처리 정보를 담은 recipe를 생성합니다


```{r}
rec <- goophi::prepForCV(data = data_train,
                         formula = formula,
                         seed = seed
                         )
```

                         
# Modeling Tab

| User Input |                                             description |
|:----------:|:-------------------------------------------------------:|
|       algo |                                            ML 모델 선택 |
|     engine |                                             engine 선택 |
|       mode |                                               mode 선택 |
|     metric |                   Best performance에 대한 평가지표 선택 |
|          v | Cross validation시 train set을 몇 번 분할할 것인지 입력 |

0) 모델 object를 저장할 빈 리스트를 생성합니다.


```{r}
models_list <- list()
```


1) logistic regression model


```{r, message=FALSE, warning=FALSE}
# User input

mode <- "classification"
algo <- "Logistic"
engine <- "glmnet" # glmnet (default), glm, stan

penalty_range_min <- "0.001"
penalty_range_max <- "1.0"
penalty_range_levels <- "5"
mixture_range_min <- "0.0"
mixture_range_max <- "1.0"
mixture_range_levels <- "5" 

v <- "2"

metric <- "roc_auc"

# Modeling

finalized <- goophi::logisticRegression(
  algo = algo,
  engine = engine,
  mode = mode,
  trainingData = data_train,
  splitedData = data_split,
  formula = formula,
  rec = rec,
  v = v,
  penaltyRangeMin = penalty_range_min,
  penaltyRangeMax = penalty_range_max,
  penaltyRangeLevels = penalty_range_levels,
  mixtureRangeMin = mixture_range_min,
  mixtureRangeMax = mixture_range_max,
  mixtureRangeLevels = mixture_range_levels,
  metric = metric
)

# Add the model to models_list
models_list[[paste0(algo, "_", engine)]] <- finalized$finalFittedModel
```


2) K Nearest Neighbor


```{r, message=FALSE, warning=FALSE}
# User input

mode <- "classification"
algo <- "KNN"
engine <- "kknn" # kknn (defualt)

neighbors_range_min <- "1" 
neighbors_range_max <- "10" 
neighbors_range_levels <- "10" 

v <- "2"

metric <- "roc_auc"

# Modeling

finalized <- goophi::KNN(
  algo = algo,
  engine = engine,
  mode = mode,
  trainingData = data_train,
  splitedData = data_split,
  formula = formula,
  rec = rec,
  v = v,
  neighborsRangeMin = neighbors_range_min,
  neighborsRangeMax = neighbors_range_max,
  neighborsRangeLevels = neighbors_range_levels,
  metric = metric
)

# Add the model to models_list
models_list[[paste0(algo, "_", engine)]] <- finalized$finalFittedModel
```


### 3) Naive Bayes


```{r, message=FALSE, warning=FALSE}
# User input

mode <- "classification"
algo <- "Naive Bayes"
engine <- "naivebayes" # klaR (default), naivebayes

smoothness_range_min <- "0.5"
smoothness_range_max <- "1.5" 
smoothness_range_levels <- "3" 
laplace_range_min <- "0.0" 
laplace_range_max <- "3.0"
laplace_range_levels <- "4"

v <- "2"

metric <- "roc_auc"

# Modeling

finalized <- goophi::naiveBayes(
  algo = algo,
  engine = engine,
  mode = mode,
  trainingData = data_train,
  splitedData = data_split,
  formula = formula,
  rec = rec,
  v = v,
  smoothnessRangeMin = smoothness_range_min,
  smoothnessRangeMax = smoothness_range_max,
  smoothnessRangeLevels = smoothness_range_levels,
  LaplaceRangeMin = laplace_range_min,
  LaplaceRangeMax = laplace_range_max,
  LaplaceRangeLevels = laplace_range_levels,
  metric = metric
)

# Add the model to models_list
models_list[[paste0(algo, "_", engine)]] <- finalized$finalFittedModel
```


### 4) Decision Tree


```{r, message=FALSE, warning=FALSE}
# User input

mode <- "classification"
algo <- "Decision Tree"
engine <- "rpart" # rpart (default), C50, partykit

tree_depth_range_min <- "1" 
tree_depth_range_max <- "15" 
tree_depth_range_levels <- "3" 
min_n_range_min <- "2" 
min_n_range_max <- "40" 
min_n_range_levels <- "3" 
cost_complexity_range_min <- "-2.0"
cost_complexity_range_max <- "-1.0" 
cost_complexity_range_levels <- "2" 

v <- "2"

metric <- "roc_auc"

# Modeling

finalized <- goophi::decisionTree(
  algo = algo,
  engine = engine,
  mode = mode,
  trainingData = data_train,
  splitedData = data_split,
  formula = formula,
  rec = rec,
  v = v,
  treeDepthRangeMin = tree_depth_range_min,
  treeDepthRangeMax = tree_depth_range_max,
  treeDepthRangeLevels = tree_depth_range_levels,
  minNRangeMin = min_n_range_min,
  minNRangeMax = min_n_range_max,
  minNRangeLevels = min_n_range_levels,
  costComplexityRangeMin = cost_complexity_range_min,
  costComplexityRangeMax = cost_complexity_range_max,
  costComplexityRangeLevels = cost_complexity_range_levels,
  metric = metric
)

# Add the model to models_list
models_list[[paste0(algo, "_", engine)]] <- finalized$finalFittedModel
```


### 5) Random Forest


```{r, message=FALSE, warning=FALSE}
# User input

mode <- "classification"
algo <- "Random Forest"
engine <- "partykit" # ranger (default), randomForest, partykit

mtry_range_min <- "1" 
mtry_range_max <- "20" 
mtry_range_levels <- "3" 
trees_range_min <- "100" 
trees_range_max <- "1000" 
trees_range_levels <- "3" 
min_n_range_min <- "2" 
min_n_range_max <- "40" 
min_n_range_levels <- "3" 

v <- "2"

metric <- "roc_auc"

# Modeling

finalized <- goophi::randomForest(
  algo = algo,
  engine = engine,
  mode = mode,
  trainingData = data_train,
  splitedData = data_split,
  formula = formula,
  rec = rec,
  v = v,
  mtryRangeMin = mtry_range_min,
  mtryRangeMax = mtry_range_max,
  mtryRangeLevels = mtry_range_levels,
  treesRangeMin = trees_range_min,
  treesRangeMax = trees_range_max,
  treesRangeLevels = trees_range_levels,
  minNRangeMin = min_n_range_min,
  minNRangeMax = min_n_range_max,
  minNRangeLevels = min_n_range_levels,
  metric = metric
)

# Add the model to models_list
models_list[[paste0(algo, "_", engine)]] <- finalized$finalFittedModel
```


### 6) XGBoost


```{r, message=FALSE, warning=FALSE}
# User input

mode <- "classification"
algo <- "XGBoost"
engine <- "xgboost" # xgboost

tree_depth_range_min <- "5" 
tree_depth_range_max <- "15" 
tree_depth_range_levels <- "3" 
trees_range_min <- "8" 
trees_range_max <- "32" 
trees_range_levels <- "3" 
learn_rate_range_min <- "-2.0" 
learn_rate_range_max <- "-1.0" 
learn_rate_range_levels <- "2" 
mtry_range_min <- "0.0" 
mtry_range_max <- "1.0" 
mtry_range_levels <- "3" 
min_n_range_min <- "2" 
min_n_range_max <- "40" 
min_n_range_levels <- "3" 
loss_reduction_range_min <- "-1.0" 
loss_reduction_range_max <- "1.0" 
loss_reduction_range_levels <- "3" 
sample_size_range_min <- "0.0" 
sample_size_range_max <- "1.0" 
sample_size_range_levels <- "3" 
stop_iter <- "30" 

v <- "2"

metric <- "roc_auc"

# Modeling

finalized <- goophi::xgBoost(
  algo = algo,
  engine = engine,
  mode = mode,
  trainingData = data_train,
  splitedData = data_split,
  formula = formula,
  rec = rec,
  v = v,
  treeDepthRangeMin = tree_depth_range_min,
  treeDepthRangeMax = tree_depth_range_max,
  treeDepthRangeLevels = tree_depth_range_levels,
  treesRangeMin = trees_range_min,
  treesRangeMax = trees_range_max,
  treesRangeLevels = trees_range_levels,
  learnRateRangeMin = learn_rate_range_min,
  learnRateRangeMax = learn_rate_range_max,
  learnRateRangeLevels = learn_rate_range_levels,
  mtryRangeMin = mtry_range_min,
  mtryRangeMax = mtry_range_max,
  mtryRangeLevels = mtry_range_levels,
  minNRangeMin = min_n_range_min,
  minNRangeMax = min_n_range_max,
  minNRangeLevels = min_n_range_levels,
  lossReductionRangeMin = loss_reduction_range_min,
  lossReductionRangeMax = loss_reduction_range_max,
  lossReductionRangeLevels = loss_reduction_range_levels,
  sampleSizeRangeMin = sample_size_range_min,
  sampleSizeRangeMax = sample_size_range_max,
  sampleSizeRangeLevels = sample_size_range_levels,
  stopIter = stop_iter,
  metric = metric
)

# Add the model to models_list
models_list[[paste0(algo, "_", engine)]] <- finalized$finalFittedModel
```



### 7) lightGBM


```{r, message=FALSE, warning=FALSE}
# User input

mode <- "classification"
algo <- "light GBM"
engine <- "lightgbm" # lightgbm

tree_depth_range_min <- "5" 
tree_depth_range_max <- "15" 
tree_depth_range_levels <- "3" 
trees_range_min <- "10" 
trees_range_max <- "100" 
trees_range_levels <- "2" 
learn_rate_range_min <- "-2.0" 
learn_rate_range_max <- "-1.0" 
learn_rate_range_levels <- "2" 
mtry_range_min <- "1" 
mtry_range_max <- "20" 
mtry_range_levels <- "3" 
min_n_range_min <- "2" 
min_n_range_max <- "40" 
min_n_range_levels <- "3" 
loss_reduction_range_min <- "-1.0" 
loss_reduction_range_max <- "1.0" 
loss_reduction_range_levels <- "3" 

v <- "2"

metric <- "roc_auc"

# Modeling

finalized <- goophi::lightGbm(
  algo = algo,
  engine = engine,
  mode = mode,
  trainingData = data_train,
  splitedData = data_split,
  formula = formula,
  rec = rec,
  v = v,
  treeDepthRangeMin = tree_depth_range_min,
  treeDepthRangeMax = tree_depth_range_max,
  treeDepthRangeLevels = tree_depth_range_levels,
  treesRangeMin = trees_range_min,
  treesRangeMax = trees_range_max,
  treesRangeLevels = trees_range_levels,
  learnRateRangeMin = learn_rate_range_min,
  learnRateRangeMax = learn_rate_range_max,
  learnRateRangeLevels = learn_rate_range_levels,
  mtryRangeMin = mtry_range_min,
  mtryRangeMax = mtry_range_max,
  mtryRangeLevels = mtry_range_levels,
  minNRangeMin = min_n_range_min,
  minNRangeMax = min_n_range_max,
  minNRangeLevels = min_n_range_levels,
  lossReductionRangeMin = loss_reduction_range_min,
  lossReductionRangeMax = loss_reduction_range_max,
  lossReductionRangeLevels = loss_reduction_range_levels,
  metric = metric
)

# Add the model to models_list
models_list[[paste0(algo, "_", engine)]] <- finalized$finalFittedModel
```

### 8) MLP


```{r, message=FALSE, warning=FALSE}
# User input
# 주석 처리한 hyperparameters는 추후 업데이트 할 engine을 위해 남겨두었습니다.

mode <- "classification"
algo <- "MLP"
engine <- "nnet" # nnet

hidden_units_range_min <- "1" 
hidden_units_range_max <- "10" 
hidden_units_range_levels <- "3" 
penalty_range_min <- "0.001" 
penalty_range_max <- "1.0" 
penalty_range_levels <- "3" 
epochs_range_min <- "10" 
epochs_range_max <- "100" 
epochs_range_levels <- "2" 

v <- "2"

metric <- "roc_auc"

# Modeling

finalized <- goophi::MLP(
  algo = algo,
  engine = engine,
  mode = mode,
  trainingData = data_train,
  splitedData = data_split,
  formula = formula,
  rec = rec,
  v = v,
  hiddenUnitsRangeMin = hidden_units_range_min,
  hiddenUnitsRangeMax = hidden_units_range_max,
  hiddenUnitsRangeLevels = hidden_units_range_levels,
  penaltyRangeMin = penalty_range_min,
  penaltyRangeMax = penalty_range_max,
  penaltyRangeLevels = penalty_range_levels,
  epochsRangeMin = epochs_range_min,
  epochsRangeMax = epochs_range_max,
  epochsRangeLevels = epochs_range_levels,
  metric = metric
)

# Add the model to models_list
models_list[[paste0(algo, "_", engine)]] <- finalized$finalFittedModel
```


## 4. Report Tab

### 1) ROC Curve

#### 유저가 선택한 모델의 ROC curve 출력


```{r, fig.width = 7, fig.height = 7}
roc_curve <- goophi::rocCurve(
  modelsList = models_list,
  targetVar = target_var
)
roc_curve
```


### 2) Confusion Matrix

#### 유저가 선택한 모델의 confusion matrix 출력

## User input


```{r}
names(models_list)
model_name <- "XGBoost_xgboost" # 원하는 모델의 이름 입력받기
```

```{r, fig.width = 7, fig.height = 6}
cm <- goophi::confusionMatrix(
  modelName = model_name,
  modelsList = models_list,
  targetVar = target_var
)
cm
```


### 3) Evaluation metrics

#### 모델 성능 평가를 위한 표 출력


```{r}
options(yardstick.event_level = "second") # 오름차순으로 factor의 level 설정된다고 가정
evalMet <- goophi::evalMetricsC(models_list, target_var)
knitr::kable(evalMet)
```

