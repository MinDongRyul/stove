% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/algorithms.R, R/fitting.R, R/report.R
\name{\%>\%}
\alias{\%>\%}
\alias{logisticRegression}
\alias{linearRegression}
\alias{KNN}
\alias{naiveBayes}
\alias{MLP}
\alias{decisionTree}
\alias{randomForest}
\alias{xgboost}
\alias{lightGbm}
\alias{kMeansClustering}
\alias{gridSearchCV}
\alias{fitBestModel}
\alias{rocCurve}
\alias{confusionMatrix}
\alias{regressionPlot}
\alias{evalMetricsC}
\alias{evalMetricsR}
\title{Logistic Regression}
\usage{
logisticRegression(
  algo = "logistic Regression",
  engine = "glm",
  mode = "classification",
  trainingData = NULL,
  splitedData = NULL,
  formula = NULL,
  rec = NULL,
  v = 5,
  penaltyRangeMin = "0.0",
  penaltyRangeMax = "1.0",
  penaltyRangeLevels = "5",
  mixtureRangeMin = "0.0",
  mixtureRangeMax = "1.0",
  mixtureRangeLevels = "5",
  metric = NULL
)

linearRegression(
  algo = "linear Regression",
  engine = "lm",
  mode = "regression",
  trainingData = NULL,
  splitedData = NULL,
  formula = NULL,
  rec = NULL,
  v = 5,
  penaltyRangeMin = "0.0",
  penaltyRangeMax = "1.0",
  penaltyRangeLevels = "5",
  mixtureRangeMin = "0.0",
  mixtureRangeMax = "1.0",
  mixtureRangeLevels = "5",
  metric = NULL
)

KNN(
  algo = "KNN",
  engine = "kknn",
  mode = "classification",
  trainingData = NULL,
  splitedData = NULL,
  formula = NULL,
  rec = NULL,
  v = 5,
  neighborsRangeMin = "2",
  neighborsRangeMax = "8",
  neighborsRangeLevels = "4",
  metric = NULL
)

naiveBayes(
  algo = "Naive Bayes",
  engine = "klaR",
  mode = "classification",
  trainingData = NULL,
  splitedData = NULL,
  formula = NULL,
  rec = NULL,
  v = 5,
  smoothnessRangeMin = "0.1",
  smoothnessRangeMax = "2",
  smoothnessRangeLevels = "5",
  LaplaceRangeMin = "0",
  LaplaceRangeMax = "3",
  LaplaceRangeLevels = "4",
  metric = NULL
)

MLP(
  algo = "MLP",
  engine = "nnet",
  mode = "classification",
  trainingData = NULL,
  splitedData = NULL,
  formula = NULL,
  rec = NULL,
  v = 5,
  hiddenUnitsRangeMin = "1",
  hiddenUnitsRangeMax = "10",
  hiddenUnitsRangeLevels = "3",
  penaltyRangeMin = "0.01",
  penaltyRangeMax = "0.5",
  penaltyRangeLevels = "3",
  epochsRangeMin = "10",
  epochsRangeMax = "100",
  epochsRangeLevels = "2",
  metric = NULL
)

decisionTree(
  algo = "MLP",
  engine = "rpart",
  mode = "classification",
  trainingData = NULL,
  splitedData = NULL,
  formula = NULL,
  rec = NULL,
  v = 5,
  treeDepthRangeMin = "3",
  treeDepthRangeMax = "10",
  treeDepthRangeLevels = "3",
  minNRangeMin = "10",
  minNRangeMax = "50",
  minNRangeLevels = "3",
  costComplexityRangeMin = "-1",
  costComplexityRangeMax = "5",
  costComplexityRangeLevels = "3",
  metric = NULL
)

randomForest(
  algo = "Random Forest",
  engine = "ranger",
  mode = "classification",
  trainingData = NULL,
  splitedData = NULL,
  formula = NULL,
  rec = NULL,
  v = 5,
  mtryRangeMin = "1",
  mtryRangeMax = "5",
  mtryRangeLevels = "3",
  treesRangeMin = "500",
  treesRangeMax = "2000",
  treesRangeLevels = "3",
  minNRangeMin = "10",
  minNRangeMax = "40",
  minNRangeLevels = "3",
  metric = NULL
)

xgboost(
  algo = "Random Forest",
  engine = "xgboost",
  mode = "classification",
  trainingData = NULL,
  splitedData = NULL,
  formula = NULL,
  rec = NULL,
  v = 5,
  treeDepthRangeMin = "3",
  treeDepthRangeMax = "6",
  treeDepthRangeLevels = "2",
  treesRangeMin = "10",
  treesRangeMax = "15",
  treesRangeLevels = "2",
  learnRateRangeMin = "0.01",
  learnRateRangeMax = "0.3",
  learnRateRangeLevels = "2",
  mtryRangeMin = "1",
  mtryRangeMax = "9",
  mtryRangeLevels = "3",
  minNRangeMin = "1",
  minNRangeMax = "10",
  minNRangeLevels = "3",
  lossReductionRangeMin = "0",
  lossReductionRangeMax = "10",
  lossReductionRangeLevels = "2",
  sampleSizeRangeMin = "0",
  sampleSizeRangeMax = "1",
  sampleSizeRangeLevels = "2",
  stopIter = "10",
  metric = NULL
)

lightGbm(
  algo = "Random Forest",
  engine = "lightgbm",
  mode = "classification",
  trainingData = NULL,
  splitedData = NULL,
  formula = NULL,
  rec = NULL,
  v = 5,
  treeDepthRangeMin = "3",
  treeDepthRangeMax = "6",
  treeDepthRangeLevels = "2",
  treesRangeMin = "10",
  treesRangeMax = "15",
  treesRangeLevels = "2",
  learnRateRangeMin = "0.01",
  learnRateRangeMax = "0.3",
  learnRateRangeLevels = "2",
  mtryRangeMin = "1",
  mtryRangeMax = "9",
  mtryRangeLevels = "3",
  minNRangeMin = "1",
  minNRangeMax = "10",
  minNRangeLevels = "3",
  lossReductionRangeMin = "0",
  lossReductionRangeMax = "10",
  lossReductionRangeLevels = "2",
  metric = NULL
)

kMeansClustering(
  data,
  maxK = "10",
  nstart = "25",
  selectOptimal = "silhouette",
  seed_num = "6471"
)

gridSearchCV(rec, model, v = "5", data, parameterGrid = 10, seed = "4814")

fitBestModel(
  gridSearchResult,
  metric,
  model,
  formula,
  trainingData,
  splitedData,
  algo
)

rocCurve(modelsList, targetVar)

confusionMatrix(modelName, modelsList, targetVar)

regressionPlot(modelName, modelsList, targetVar)

evalMetricsC(modelsList, targetVar)

evalMetricsR(modelsList, targetVar)
}
\arguments{
\item{algo}{algo}

\item{engine}{engine}

\item{mode}{mode}

\item{trainingData}{trainingData}

\item{splitedData}{splitedData}

\item{formula}{formula}

\item{rec}{rec}

\item{v}{v-fold CV}

\item{penaltyRangeMin}{penaltyRangeMin}

\item{penaltyRangeMax}{penaltyRangeMax}

\item{penaltyRangeLevels}{penaltyRangeLevels}

\item{mixtureRangeMin}{mixtureRangeMin}

\item{mixtureRangeMax}{mixtureRangeMax}

\item{mixtureRangeLevels}{mixtureRangeLevels}

\item{metric}{metric}

\item{neighborsRangeMin}{neighborsRangeMin}

\item{neighborsRangeMax}{neighborsRangeMax}

\item{neighborsRangeLevels}{neighborsRangeLevels}

\item{smoothnessRangeMin}{smoothnessRangeMin}

\item{smoothnessRangeMax}{smoothnessRangeMax}

\item{smoothnessRangeLevels}{smoothnessRangeLevels}

\item{LaplaceRangeMin}{LaplaceRangeMin}

\item{LaplaceRangeMax}{LaplaceRangeMax}

\item{LaplaceRangeLevels}{LaplaceRangeLevels}

\item{hiddenUnitsRangeMin}{hiddenUnitsRangeMin}

\item{hiddenUnitsRangeMax}{hiddenUnitsRangeMax}

\item{hiddenUnitsRangeLevels}{hiddenUnitsRangeLevels}

\item{epochsRangeMin}{epochsRangeMin}

\item{epochsRangeMax}{epochsRangeMax}

\item{epochsRangeLevels}{epochsRangeLevels}

\item{treeDepthRangeMin}{treeDepthRangeMin}

\item{treeDepthRangeMax}{treeDepthRangeMax}

\item{treeDepthRangeLevels}{treeDepthRangeLevels}

\item{minNRangeMin}{minNRangeMin}

\item{minNRangeMax}{minNRangeMax}

\item{minNRangeLevels}{minNRangeLevels}

\item{costComplexityRangeMin}{costComplexityRangeMin}

\item{costComplexityRangeMax}{costComplexityRangeMax}

\item{costComplexityRangeLevels}{costComplexityRangeLevels}

\item{mtryRangeMin}{mtryRangeMin}

\item{mtryRangeMax}{mtryRangeMax}

\item{mtryRangeLevels}{mtryRangeLevels}

\item{treesRangeMin}{treesRangeMin}

\item{treesRangeMax}{treesRangeMax}

\item{treesRangeLevels}{treesRangeLevels}

\item{learnRateRangeMin}{learnRateRangeMin}

\item{learnRateRangeMax}{learnRateRangeMax}

\item{learnRateRangeLevels}{learnRateRangeLevels}

\item{data}{data}

\item{maxK}{maxK}

\item{nstart}{nstart}

\item{selectOptimal}{selectOptimal}

\item{seed_num}{seed_num}

\item{model}{model}

\item{seed}{seed}

\item{gridSearchResult}{gridSearchResult}

\item{modelsList}{modelsList}

\item{targetVar}{targetVar}

\item{modelName}{modelName}

\item{dropoutRangeMin}{dropoutRangeMin}

\item{dropoutRangeMax}{dropoutRangeMax}

\item{dropoutRangeLevels}{dropoutRangeLevels}

\item{activation}{activation}

\item{parameter_grid}{parameter_grid}

\item{models_list}{models_list}
}
\description{
Logistic Regression

Linear Regression

K-Nearest Neighbors

Naive Bayes

neural network

Decision Tree

Random Forest

XGBoost

Light GBM

K means clustering

Grid Search with cross validation

fitting in best model

AUC-ROC Curve

Confusion matrix

Regression plot

Evaluation metrics for Classification

Evaluation metrics for Regression
}
\details{
로지스틱 회귀 알고리즘 함수. 예측 변수들이 정규분포를 따르지 않아도 사용할 수 있습니다.
그러나 이 알고리즘은 결과 변수가 선형적으로 구분되며, 예측 변수들의 값이 결과 변수와 선형 관계를
갖는다고 가정합니다. 만약 데이터가 이 가정을 충족하지 않는 경우 성능이 저하될 수 있습니다.
hyperparameters: penalty, mixture

Linear Regression
hyperparameters: penalty, mixture

KNN 알고리즘 함수.
데이터로부터 거리가 가까운 K개의 다른 데이터의 레이블을 참조하여 분류하는 알고리즘
hyperparameters: neighbors

Naive Bayes
hyperparameters: smoothness, Laplace

neural network 알고리즘 함수.
neural network 모델은 생물학적인 뉴런을 수학적으로 모델링한 것.
여러개의 뉴런으로부터 입력값을 받아서 세포체에 저장하다가 자신의 용량을 넘어서면 외부로 출력값을 내보내는 것처럼,
인공신경망 뉴런은 여러 입력값을 받아서 일정 수준이 넘어서면 활성화되어 출력값을 내보낸다.
hyperparameters: hidden_units, penalty, dropout, epochs, activation, learn_rate

의사결정나무 알고리즘 함수. 의사 결정 규칙 (Decision rule)을 나무 형태로 분류해 나가는 분석 기법을 말합니다.
hyperparameters: cost_complexity, tree_depth, min_n

랜덤 포레스트 알고리즘 함수. 여러개의 Decision Tree를 형성.
새로운 데이터 포인트를 각 트리에 동시에 통과 시켜 각 트리가 분류한 결과에서 투표를 실시하여
가장 많이 득표한 결과를 최종 분류 결과로 선택
hyperparameters: trees, min_n, mtry

XGBoost
hyperparameters: mtry, min_n, tree_depth, loss_reduction, learn_rate, sample_size

Light GBM
install treesnip package by: remotes::install_github("curso-r/treesnip")
hyperparameters: mtry, min_n, tree_depth, loss_reduction, learn_rate, sample_size

K means clustering
selectOptimal: silhouette, gap_stat
hyperparameters: maxK, nstart

Grid Search with cross validation

fitting in best model

AUC-ROC Curve

Confusion matrix

Regression plot

Evaluation metrics for Classification

Evaluation metrics for Regression
}
