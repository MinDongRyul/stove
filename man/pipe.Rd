% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/algorithms.R, R/fitting.R, R/report.R
\name{\%>\%}
\alias{\%>\%}
\alias{logisticRegression}
\alias{linearRegression}
\alias{KNN}
\alias{naiveBayes}
\alias{decisionTree}
\alias{randomForest}
\alias{xgboost}
\alias{lightGbm}
\alias{MLP}
\alias{kMeansClustering}
\alias{gridSearchCV}
\alias{fitBestModel}
\alias{rocCurve}
\alias{confusionMatrix}
\alias{regressionPlot}
\alias{evalMetricsC}
\alias{evalMetricsR}
\title{Logistic Regression}
\usage{
logisticRegression(
  algo = "logistic Regression",
  engine = "glm",
  mode = "classification",
  trainingData = NULL,
  splitedData = NULL,
  formula = NULL,
  rec = NULL,
  v = "5",
  penaltyRangeMin = "0.0",
  penaltyRangeMax = "20.0",
  penaltyRangeLevels = "5",
  mixtureRangeMin = "0.0",
  mixtureRangeMax = "1.0",
  mixtureRangeLevels = "5",
  metric = NULL
)

linearRegression(
  algo = "linear Regression",
  engine = "lm",
  mode = "regression",
  trainingData = NULL,
  splitedData = NULL,
  formula = NULL,
  rec = NULL,
  v = "5",
  penaltyRangeMin = "0.0",
  penaltyRangeMax = "20.0",
  penaltyRangeLevels = "5",
  mixtureRangeMin = "0.0",
  mixtureRangeMax = "1.0",
  mixtureRangeLevels = "5",
  metric = NULL
)

KNN(
  algo = "KNN",
  engine = "kknn",
  mode = "classification",
  trainingData = NULL,
  splitedData = NULL,
  formula = NULL,
  rec = NULL,
  v = "5",
  neighborsRangeMin = "2",
  neighborsRangeMax = "20",
  neighborsRangeLevels = "10",
  metric = NULL
)

naiveBayes(
  algo = "Naive Bayes",
  engine = "klaR",
  mode = "classification",
  trainingData = NULL,
  splitedData = NULL,
  formula = NULL,
  rec = NULL,
  v = "5",
  smoothnessRangeMin = "0.001",
  smoothnessRangeMax = "10",
  smoothnessRangeLevels = "5",
  LaplaceRangeMin = "0",
  LaplaceRangeMax = "50",
  LaplaceRangeLevels = "6",
  metric = NULL
)

decisionTree(
  algo = "Decision Tree",
  engine = "rpart",
  mode = "classification",
  trainingData = NULL,
  splitedData = NULL,
  formula = NULL,
  rec = NULL,
  v = "5",
  treeDepthRangeMin = "10",
  treeDepthRangeMax = "30",
  treeDepthRangeLevels = "3",
  minNRangeMin = "2",
  minNRangeMax = "500",
  minNRangeLevels = "3",
  costComplexityRangeMin = "0",
  costComplexityRangeMax = "1",
  costComplexityRangeLevels = "3",
  metric = NULL
)

randomForest(
  algo = "Random Forest",
  engine = "ranger",
  mode = "classification",
  trainingData = NULL,
  splitedData = NULL,
  formula = NULL,
  rec = NULL,
  v = "5",
  mtryRangeMin = "1",
  mtryRangeMax = "20",
  mtryRangeLevels = "3",
  treesRangeMin = "100",
  treesRangeMax = "2000",
  treesRangeLevels = "3",
  minNRangeMin = "2",
  minNRangeMax = "500",
  minNRangeLevels = "3",
  metric = NULL
)

xgboost(
  algo = "Random Forest",
  engine = "xgboost",
  mode = "classification",
  trainingData = NULL,
  splitedData = NULL,
  formula = NULL,
  rec = NULL,
  v = "5",
  treeDepthRangeMin = "2",
  treeDepthRangeMax = "50",
  treeDepthRangeLevels = "3",
  treesRangeMin = "10",
  treesRangeMax = "50",
  treesRangeLevels = "2",
  learnRateRangeMin = "0.01",
  learnRateRangeMax = "0.3",
  learnRateRangeLevels = "2",
  mtryRangeMin = "1",
  mtryRangeMax = "20",
  mtryRangeLevels = "3",
  minNRangeMin = "1",
  minNRangeMax = "500",
  minNRangeLevels = "3",
  lossReductionRangeMin = "0",
  lossReductionRangeMax = "1",
  lossReductionRangeLevels = "3",
  sampleSizeRangeMin = "0",
  sampleSizeRangeMax = "1",
  sampleSizeRangeLevels = "3",
  stopIter = "30",
  metric = NULL
)

lightGbm(
  algo = "Random Forest",
  engine = "lightgbm",
  mode = "classification",
  trainingData = NULL,
  splitedData = NULL,
  formula = NULL,
  rec = NULL,
  v = "5",
  treeDepthRangeMin = "2",
  treeDepthRangeMax = "30",
  treeDepthRangeLevels = "3",
  treesRangeMin = "10",
  treesRangeMax = "50",
  treesRangeLevels = "2",
  learnRateRangeMin = "0.01",
  learnRateRangeMax = "0.3",
  learnRateRangeLevels = "2",
  mtryRangeMin = "1",
  mtryRangeMax = "20",
  mtryRangeLevels = "3",
  minNRangeMin = "1",
  minNRangeMax = "500",
  minNRangeLevels = "3",
  lossReductionRangeMin = "0",
  lossReductionRangeMax = "1",
  lossReductionRangeLevels = "3",
  metric = NULL
)

MLP(
  algo = "MLP",
  engine = "nnet",
  mode = "classification",
  trainingData = NULL,
  splitedData = NULL,
  formula = NULL,
  rec = NULL,
  v = "5",
  hiddenUnitsRangeMin = "1",
  hiddenUnitsRangeMax = "10",
  hiddenUnitsRangeLevels = "3",
  penaltyRangeMin = "0.01",
  penaltyRangeMax = "0.5",
  penaltyRangeLevels = "3",
  epochsRangeMin = "10",
  epochsRangeMax = "100",
  epochsRangeLevels = "2",
  metric = NULL
)

kMeansClustering(
  data,
  maxK = "15",
  nStart = "25",
  iterMax = "10",
  nBoot = "100",
  algorithm = "Hartigan-Wong",
  selectOptimal = "silhouette",
  seedNum = "6471"
)

gridSearchCV(rec, model, v = "5", data, parameterGrid = 10, seed = "4814")

fitBestModel(
  gridSearchResult,
  metric,
  model,
  formula,
  trainingData,
  splitedData,
  algo
)

rocCurve(modelsList, targetVar)

confusionMatrix(modelName, modelsList, targetVar)

regressionPlot(modelName, modelsList, targetVar)

evalMetricsC(modelsList, targetVar)

evalMetricsR(modelsList, targetVar)
}
\arguments{
\item{algo}{algo}

\item{engine}{engine}

\item{mode}{mode}

\item{trainingData}{trainingData}

\item{splitedData}{splitedData}

\item{formula}{formula}

\item{rec}{rec}

\item{v}{v-fold CV}

\item{penaltyRangeMin}{penaltyRangeMin}

\item{penaltyRangeMax}{penaltyRangeMax}

\item{penaltyRangeLevels}{penaltyRangeLevels}

\item{mixtureRangeMin}{mixtureRangeMin}

\item{mixtureRangeMax}{mixtureRangeMax}

\item{mixtureRangeLevels}{mixtureRangeLevels}

\item{metric}{metric}

\item{neighborsRangeMin}{neighborsRangeMin}

\item{neighborsRangeMax}{neighborsRangeMax}

\item{neighborsRangeLevels}{neighborsRangeLevels}

\item{smoothnessRangeMin}{smoothnessRangeMin}

\item{smoothnessRangeMax}{smoothnessRangeMax}

\item{smoothnessRangeLevels}{smoothnessRangeLevels}

\item{LaplaceRangeMin}{LaplaceRangeMin}

\item{LaplaceRangeMax}{LaplaceRangeMax}

\item{LaplaceRangeLevels}{LaplaceRangeLevels}

\item{treeDepthRangeMin}{treeDepthRangeMin}

\item{treeDepthRangeMax}{treeDepthRangeMax}

\item{treeDepthRangeLevels}{treeDepthRangeLevels}

\item{minNRangeMin}{minNRangeMin}

\item{minNRangeMax}{minNRangeMax}

\item{minNRangeLevels}{minNRangeLevels}

\item{costComplexityRangeMin}{costComplexityRangeMin}

\item{costComplexityRangeMax}{costComplexityRangeMax}

\item{costComplexityRangeLevels}{costComplexityRangeLevels}

\item{mtryRangeMin}{mtryRangeMin}

\item{mtryRangeMax}{mtryRangeMax}

\item{mtryRangeLevels}{mtryRangeLevels}

\item{treesRangeMin}{treesRangeMin}

\item{treesRangeMax}{treesRangeMax}

\item{treesRangeLevels}{treesRangeLevels}

\item{learnRateRangeMin}{learnRateRangeMin}

\item{learnRateRangeMax}{learnRateRangeMax}

\item{learnRateRangeLevels}{learnRateRangeLevels}

\item{hiddenUnitsRangeMin}{hiddenUnitsRangeMin}

\item{hiddenUnitsRangeMax}{hiddenUnitsRangeMax}

\item{hiddenUnitsRangeLevels}{hiddenUnitsRangeLevels}

\item{epochsRangeMin}{epochsRangeMin}

\item{epochsRangeMax}{epochsRangeMax}

\item{epochsRangeLevels}{epochsRangeLevels}

\item{data}{data}

\item{maxK}{maxK}

\item{algorithm}{algorithm}

\item{selectOptimal}{selectOptimal}

\item{model}{model}

\item{seed}{seed}

\item{gridSearchResult}{gridSearchResult}

\item{modelsList}{modelsList}

\item{targetVar}{targetVar}

\item{modelName}{modelName}

\item{dropoutRangeMin}{dropoutRangeMin}

\item{dropoutRangeMax}{dropoutRangeMax}

\item{dropoutRangeLevels}{dropoutRangeLevels}

\item{activation}{activation}

\item{nstart}{nstart}

\item{seed_num}{seed_num}

\item{iter.max}{iter.max}

\item{parameter_grid}{parameter_grid}

\item{models_list}{models_list}
}
\description{
Logistic Regression

Linear Regression

K-Nearest Neighbors

Naive Bayes

Decision Tree

Random Forest

XGBoost

Light GBM

neural network

K means clustering

Grid Search with cross validation

fitting in best model

AUC-ROC Curve

Confusion matrix

Regression plot

Evaluation metrics for Classification

Evaluation metrics for Regression
}
\details{
로지스틱 회귀 알고리즘 함수. 예측 변수들이 정규분포를 따르지 않아도 사용할 수 있습니다.
그러나 이 알고리즘은 결과 변수가 선형적으로 구분되며, 예측 변수들의 값이 결과 변수와 선형 관계를
갖는다고 가정합니다. 만약 데이터가 이 가정을 충족하지 않는 경우 성능이 저하될 수 있습니다.
hyperparameters: penalty, mixture

Linear Regression
hyperparameters: penalty, mixture

KNN 알고리즘 함수.
데이터로부터 거리가 가까운 K개의 다른 데이터의 레이블을 참조하여 분류하는 알고리즘
hyperparameters: neighbors

Naive Bayes
hyperparameters: smoothness, Laplace

의사결정나무 알고리즘 함수. 의사 결정 규칙 (Decision rule)을 나무 형태로 분류해 나가는 분석 기법을 말합니다.
hyperparameters:
tree_depth: 최종 예측값에 다다르기까지 몇 번 트리를 분할할지 설정합니다.
min_n: 트리를 분할하기 위해 필요한 관측값의 최소 개수를 설정합니다.
cost_complexity: 트리 분할을 위해 필요한 비용을 설정합니다. 0일 경우, 가능한 모든 분할이 수행됩니다.

랜덤 포레스트 알고리즘 함수. 여러개의 Decision Tree를 형성.
새로운 데이터 포인트를 각 트리에 동시에 통과 시켜 각 트리가 분류한 결과에서 투표를 실시하여
가장 많이 득표한 결과를 최종 분류 결과로 선택
hyperparameters:
trees: 결정트리의 개수를 지정합니다.
min_n: 트리를 분할하기 위해 필요한 관측값의 최소 개수를 설정합니다.
mtry: 트리를 분할하기 위해 필요한 feature의 수를 설정합니다.

XGBoost
hyperparameters:
tree_depth: 최종 예측값에 다다르기까지 몇 번 트리를 분할할지 설정합니다.
trees:
learn_rate,
mtry,
min_n,
loss_reduction,
sample_size
stop_iter

Light GBM
install treesnip package by: remotes::install_github("curso-r/treesnip")
hyperparameters: mtry, min_n, tree_depth, loss_reduction, learn_rate, sample_size

neural network 알고리즘 함수.
neural network 모델은 생물학적인 뉴런을 수학적으로 모델링한 것.
여러개의 뉴런으로부터 입력값을 받아서 세포체에 저장하다가 자신의 용량을 넘어서면 외부로 출력값을 내보내는 것처럼,
인공신경망 뉴런은 여러 입력값을 받아서 일정 수준이 넘어서면 활성화되어 출력값을 내보낸다.
hyperparameters: hidden_units, penalty, dropout, epochs, activation, learn_rate

K means clustering
selectOptimal: silhouette, gap_stat
hyperparameters: maxK, nstart

Grid Search with cross validation // workflows rsample tune

fitting in best model // tune workflows

AUC-ROC Curve // RColorBrewer cowplot ggplot2 yardstick

Confusion matrix // yardstick tune ggplot2

Regression plot // yardstick tune ggplot2 ggrepel

Evaluation metrics for Classification // yardstick tune ggplot2 data.table

Evaluation metrics for Regression // yardstick tune ggplot2 data.table
}
