devtools::document()
goophi::logisticRegression
?goophi::logisticRegression
library(dotwhisker)  # for visualizing regression results
library(tidymodels)  # for the parsnip package, along with the rest of tidymodels
# Helper packages
library(readr)       # for importing data
library(broom.mixed) # for converting bayesian models to tidy tibbles
library(dotwhisker)  # for visualizing regression results
install.packages("broom.mixed")
install.packages("dotwhisker")
library(broom.mixed) # for converting bayesian models to tidy tibbles
library(dotwhisker)  # for visualizing regression results
urchins <-
# Data were assembled for a tutorial
# at https://www.flutterbys.com.au/stats/tut/tut7.5a.html
read_csv("https://tidymodels.org/start/models/urchins.csv") %>%
# Change the names to be a little more verbose
setNames(c("food_regime", "initial_volume", "width")) %>%
# Factors are very helpful for modeling, so we convert one column
mutate(food_regime = factor(food_regime, levels = c("Initial", "Low", "High")))
urchins
ggplot(urchins,
aes(x = initial_volume,
y = width,
group = food_regime,
col = food_regime)) +
geom_point() +
geom_smooth(method = lm, se = FALSE) +
scale_color_viridis_d(option = "plasma", end = .7)
linear_reg()
parsnip::linear_reg()
# parsnip: model의 functional form  제공
# 사용가능한 computational engine: https://parsnip.tidymodels.org/reference/linear_reg.html
parsnip::linear_reg() %>%
set_engine("keras")
lm_fit
lm_fit <-
lm_mod %>%
fit(width ~ initial_volume * food_regime, data = urchins)
lm_fit
lm_fit <-
lm_mod %>%
fit(width ~ initial_volume * food_regime, data = urchins)
# parsnip: model의 functional form  제공
# 사용가능한 computational engine: https://parsnip.tidymodels.org/reference/linear_reg.html
lm_mod <- parsnip::linear_reg() %>%
set_engine("keras")
lm_fit <-
lm_mod %>%
fit(width ~ initial_volume * food_regime, data = urchins)
lm_fit
# parsnip: model의 functional form  제공
# 사용가능한 computational engine: https://parsnip.tidymodels.org/reference/linear_reg.html
lm_mod <- parsnip::linear_reg() %>%
set_engine("glm") ## keras engine을 이용하려면 패키지 설치 필요. 사용할 수 있는 engine에 필요한 패키지 전부 import??
lm_fit <-
lm_mod %>%
fit(width ~ initial_volume * food_regime, data = urchins)
lm_fit
?tidy
?fit
lm_fit <-
lm_mod %>%
parsnip::fit(width ~ initial_volume * food_regime, data = urchins)
lm_fit
parsnip::tidy(lm_fit)
tidy(lm_fit) %>%
dwplot(dot_args = list(size = 2, color = "black"),
whisker_args = list(color = "black"),
vline = geom_vline(xintercept = 0, colour = "grey50", linetype = 2))
new_points <- expand.grid(initial_volume = 20,
food_regime = c("Initial", "Low", "High"))
new_points
lm_fit
library(tidymodels)  # for the parsnip package, along with the rest of tidymodels
# Helper packages
library(readr)       # for importing data
library(broom.mixed) # for converting bayesian models to tidy tibbles
library(dotwhisker)  # for visualizing regression results
# data 가져오기
urchins <-
# Data were assembled for a tutorial
# at https://www.flutterbys.com.au/stats/tut/tut7.5a.html
read_csv("https://tidymodels.org/start/models/urchins.csv") %>%
# Change the names to be a little more verbose
setNames(c("food_regime", "initial_volume", "width")) %>%
# Factors are very helpful for modeling, so we convert one column
mutate(food_regime = factor(food_regime, levels = c("Initial", "Low", "High")))
urchins
# scatter-line plot
ggplot(urchins,
aes(x = initial_volume,
y = width,
group = food_regime,
col = food_regime)) +
geom_point() +
geom_smooth(method = lm, se = FALSE) +
scale_color_viridis_d(option = "plasma", end = .7)
################################ parsnip: model의 functional form  제공
# 사용가능한 computational engine: https://parsnip.tidymodels.org/reference/linear_reg.html
lm_mod <- parsnip::linear_reg() %>%
set_engine("glm") ## keras engine을 이용하려면 패키지 설치 필요. 사용할 수 있는 engine에 필요한 패키지 전부 import??
lm_fit <-
lm_mod %>%
parsnip::fit(width ~ initial_volume * food_regime, data = urchins)
lm_fit
parsnip::tidy(lm_fit)
lm_fit
lm_fit$fit
str(lm_fit)
?expand.grid
new_points
View(urchins)
mean_pred <- predict(lm_fit, new_data = new_points)
mean_pred
?dwplot
?dwplot
?predict
mean_pred <- stats::predict(lm_fit, new_data = new_points)
mean_pred
mean_pred <- GPfit::predict(lm_fit, new_data = new_points)
mean_pred
stats
stats
mean_pred <- stats::predict(lm_fit, new_data = new_points)
mean_pred
prior_dist <- rstanarm::student_t(df = 1)
new_points <- expand.grid(initial_volume = 20,
food_regime = c("Initial", "Low", "High"))
new_points
mean_pred <- stats::predict(lm_fit, new_data = new_points)
mean_pred
conf_int_pred <- predict(lm_fit,
new_data = new_points,
type = "conf_int")
conf_int_pred
# Now combine:
plot_data <-
new_points %>%
bind_cols(mean_pred) %>%
bind_cols(conf_int_pred)
# and plot:
ggplot(plot_data, aes(x = food_regime)) +
geom_point(aes(y = .pred)) +
geom_errorbar(aes(ymin = .pred_lower,
ymax = .pred_upper),
width = .2) +
labs(y = "urchin size")
plot_data
library(rstanarm)
install.packages("rstanarm")
## bayesian analysis는 결과가 다를까?
library(rstanarm)
# set the prior distribution
prior_dist <- rstanarm::student_t(df = 1)
set.seed(123)
# make the parsnip model
bayes_mod <-
linear_reg() %>%
set_engine("stan",
prior_intercept = prior_dist,
prior = prior_dist)
# train the model
bayes_fit <-
bayes_mod %>%
fit(width ~ initial_volume * food_regime, data = urchins)
print(bayes_fit, digits = 5)
tidy(bayes_fit, conf.int = TRUE)
bayes_plot_data <-
new_points %>%
bind_cols(predict(bayes_fit, new_data = new_points)) %>%
bind_cols(predict(bayes_fit, new_data = new_points, type = "conf_int"))
ggplot(bayes_plot_data, aes(x = food_regime)) +
geom_point(aes(y = .pred)) +
geom_errorbar(aes(ymin = .pred_lower, ymax = .pred_upper), width = .2) +
labs(y = "urchin size") +
ggtitle("Bayesian model with t(1) prior distribution")
# and plot:
ggplot(plot_data, aes(x = food_regime)) +
geom_point(aes(y = .pred)) +
geom_errorbar(aes(ymin = .pred_lower,
ymax = .pred_upper),
width = .2) +
labs(y = "urchin size")
ggplot(bayes_plot_data, aes(x = food_regime)) +
geom_point(aes(y = .pred)) +
geom_errorbar(aes(ymin = .pred_lower, ymax = .pred_upper), width = .2) +
labs(y = "urchin size") +
ggtitle("Bayesian model with t(1) prior distribution")
#### modeling ####
library(tidymodels) # for the rsample package, along with the rest of tidymodels
# Helper packages
library(modeldata)  # for the cells data
data(cells, package = "modeldata")
cells
cells %>%
count(class) %>%
mutate(prop = n/sum(n))
cells
set.seed(123)
cell_split <- rsample::initial_split(cells %>% select(-case),
strata = class)
rsample
cell_split
str(cells)
cell_train <- rsample::training(cell_split)
cell_test  <- rsample::testing(cell_split)
nrow(cell_train)
nrow(cell_train)/nrow(cells)
cell_train %>%
count(class) %>%
mutate(prop = n/sum(n))
# test set proportions by class
cell_test %>%
count(class) %>%
mutate(prop = n/sum(n))
rf_mod <-
rand_forest(trees = 1000) %>%
set_engine("ranger") %>%
set_mode("classification")
rf_fit
set.seed(234)
rf_fit <-
rf_mod %>%
fit(class ~ ., data = cell_train)
rf_fit
install.packages("ranger")
set.seed(234)
rf_fit <-
rf_mod %>%
fit(class ~ ., data = cell_train)
rf_fit
f_training_pred <-
predict(rf_fit, cell_train) %>%
bind_cols(predict(rf_fit, cell_train, type = "prob")) %>%
# Add the true outcome data back in
bind_cols(cell_train %>%
select(class))
rf_training_pred %>%                # training set predictions
roc_auc(truth = class, .pred_PS)
## ESTIMATING PERFORMANCE
rf_training_pred <-
predict(rf_fit, cell_train) %>%
bind_cols(predict(rf_fit, cell_train, type = "prob")) %>%
# Add the true outcome data back in
bind_cols(cell_train %>%
select(class))
rf_training_pred %>%                # training set predictions
roc_auc(truth = class, .pred_PS)
rf_testing_pred <-
predict(rf_fit, cell_test) %>%
bind_cols(predict(rf_fit, cell_test, type = "prob")) %>%
bind_cols(cell_test %>% select(class))
f_testing_pred %>%                   # test set predictions
roc_auc(truth = class, .pred_PS)
rf_testing_pred %>%                   # test set predictions
roc_auc(truth = class, .pred_PS)
folds
folds <- vfold_cv(cell_train, v = 10)
folds
set.seed(345)
folds <- vfold_cv(cell_train, v = 10)
folds
rf_fit_rs
rf_wf <-
workflow() %>%
add_model(rf_mod) %>%
add_formula(class ~ .)
set.seed(456)
rf_fit_rs <-
rf_wf %>%
fit_resamples(folds)
rf_fit_rs
collect_metrics(rf_fit_rs)
library(tidymodels)  # for the tune package, along with the rest of tidymodels
# Helper packages
library(rpart.plot)  # for visualizing a decision tree
library(vip)         # for variable importance plots
rf_fit_rs
data(cells, package = "modeldata")
cells
set.seed(123)
cell_split <- initial_split(cells %>% select(-case),
strata = class)
cell_train <- training(cell_split)
cell_test  <- testing(cell_split)
tune_spec <-
decision_tree(
cost_complexity = tune(),
tree_depth = tune()
) %>%
set_engine("rpart") %>%
set_mode("classification")
tune_spec
?grid_regular
tune_spec
tree_grid <- dials::grid_regular(cost_complexity(),
tree_depth(),
levels = 5)
set.seed(234)
cell_folds <- vfold_cv(cell_train)
set.seed(345)
tree_wf <- workflow() %>%
add_model(tune_spec) %>%
add_formula(class ~ .)
tree_res <-
tree_wf %>%
tune_grid(
resamples = cell_folds,
grid = tree_grid
)
tree_res
final_wf <-
tree_wf %>%
finalize_workflow(best_tree)
best_tree <- tree_res %>%
select_best("accuracy")
final_wf <-
tree_wf %>%
finalize_workflow(best_tree)
tune_spec <-
decision_tree(
cost_complexity = tune(),
tree_depth = tune()
) %>%
set_engine("rpart, keras") %>%
set_mode("classification")
tune_spec <-
decision_tree(
cost_complexity = tune(),
tree_depth = tune()
) %>%
set_engine(c("rpart", "keras")) %>%
set_mode("classification")
library(tidymodels)  # for the tune package, along with the rest of tidymodels
library(dplyr)
library(recipes)
library(parsnip)
library(tune)
library(rsample)
# Helper packages
library(rpart.plot)  # for visualizing a decision tree
library(vip)         # for variable importance plots
data(cells, package = "modeldata")
cells
titanicTib <- tibble::as_tibble(titanic_train)
?as_tibble
# install.packages("titanic")
data(titanic_train, package = "titanic")
titanicTib <- tibble::as_tibble(titanic_train)
# install.packages("titanic")
data(titanic_train, package = "titanic")
cleaned_data <- tibble::as_tibble(titanic_train)
library(tidymodels)  # for the tune package, along with the rest of tidymodels
library(dplyr)
library(recipes)
library(parsnip)
library(tune)
library(rsample)
# install.packages("titanic")
data(titanic_train, package = "titanic")
cleaned_data <- tibble::as_tibble(titanic_train)
is.na(cleaned_data)
is.na(cleaned_data) == TRUE
userImputation = TRUE ## 유저가 imputation을 선택해도 na가 없으면 실행되지 않도록
userScaling = TRUE
userPca = TRUE
set.seed(42)
set.seed(1234)
if (userImputation == TRUE & is.na(cleaned_data)){
print("--------------Imputation has performed--------------")
}
if (userImputation == TRUE && is.na(cleaned_data)){
print("--------------Imputation has performed--------------")
}
if (userImputation == TRUE && any(is.na(cleaned_data))){
print("--------------Imputation has performed--------------")
}
if (userImputation == TRUE & any(is.na(cleaned_data))){
print("--------------Imputation has performed--------------")
}
cleaned_data
?initial_split
set.seed(1234) # fix seed
data_split <- rsample::initial_split(cleaned_data, strata = Survived)
data_train <- rsample::training(data_split)
data_test  <- rsample::testing(data_split)
nrow(data_train)
nrow(data_train)/nrow(cleaned_data)
# training set proportions by class
data_train %>%
count(Survived) %>%
mutate(prop = n/sum(n))
# test set proportions by class
data_test %>%
count(Survived) %>%
mutate(prop = n/sum(n))
rec <- recipe(Survived ~ ., data = data_train)
rec
?vfold_cv
# create CV object from training data
data_train_cv <- rsample::vfold_cv(data_train)
data_train_cv
rec <- recipe(Survived ~ ., data = data_train_cv)
# 함수 추가 시 roxygen 주석을 포함시켜 작성하고, 아래 코드로 주석을 .Rd 파일로 전환 및 NAMESPACE에 추가
devtools::document()
data_train <- goophi::trainTestSplit(data, strata)$train
strata <- "Survived"
data_train <- goophi::trainTestSplit(data, strata)$train
data_train <- goophi::trainTestSplit(data, eval(strata))$train
data_train <- goophi::trainTestSplit(data, eval(parse(strata)))$train
data_train <- goophi::trainTestSplit(data, eval(parse(text = strata)))$train
cleaned_data
#### train-test split ( goophi::trainTestSplit(data, strata); tibble -> list(train, test)) ####
data_split <- rsample::initial_split(cleaned_data %>%
select(-PassengerId), strata = Survived)
data_split
#### train-test split ( goophi::trainTestSplit(data, strata); tibble -> list(train, test)) ####
data_split <- rsample::initial_split(cleaned_data %>%
select(-PassengerId), strata = Survived)
data_train <- rsample::training(data_split)
data_test  <- rsample::testing(data_split)
data_train
data_train <- goophi::trainTestSplit(data, data[[strata]])$train
data_train <- goophi::trainTestSplit(cleaned_data, strata = data[[strata]])$train
data_train <- goophi::trainTestSplit(cleaned_data, strata = cleaned_data[[strata]])$train
data_test <- goophi::trainTestSplit(cleaned_data, strata)$test
# install.packages("titanic")
data(titanic_train, package = "titanic")
cleaned_data <- tibble::as_tibble(titanic_train) %>%
select(-PassengerId)
set.seed(1234) # fix seed
#### train-test split ( goophi::trainTestSplit(data, strata); tibble -> list(train, test)) ####
data_split <- rsample::initial_split(cleaned_data, strata = Survived)
data_train <- rsample::training(data_split)
data_test  <- rsample::testing(data_split)
data_train
data_test
data_train <- goophi::trainTestSplit(cleaned_data, strata = cleaned_data[[strata]])$train
data_test <- goophi::trainTestSplit(cleaned_data, strata = cleaned_data[[strata]])$test
data_train
goophi::trainTestSplit(cleaned_data, strata = cleaned_data[[strata]])
data_train <- goophi::trainTestSplit(cleaned_data, strata = cleaned_data[[strata]])$1
data_train <- goophi::trainTestSplit(cleaned_data, strata = cleaned_data[[strata]])$[1]
data_train <- goophi::trainTestSplit(cleaned_data, strata = cleaned_data[[strata]])$[[1]]
data_train <- goophi::trainTestSplit(cleaned_data, strata = cleaned_data[[strata]])[1]
data_train <- goophi::trainTestSplit(cleaned_data, strata = cleaned_data[[strata]])[1]
data_test <- goophi::trainTestSplit(cleaned_data, strata = cleaned_data[[strata]])[2]
data_train
nrow(data_train)
nrow(data_train)/nrow(cleaned_data)
data_train
data_train <- goophi::trainTestSplit(cleaned_data, strata = cleaned_data[[strata]])[[1]]
data_train
nrow(data_train)
nrow(data_train)/nrow(cleaned_data)
data_test <- goophi::trainTestSplit(cleaned_data, strata = cleaned_data[[strata]])[[2]]
nrow(data_train)
nrow(data_train)/nrow(cleaned_data)
# training set proportions by class
data_train %>%
count(Survived) %>%
mutate(prop = n/sum(n))
# test set proportions by class
data_test %>%
count(Survived) %>%
mutate(prop = n/sum(n))
# 함수 추가 시 roxygen 주석을 포함시켜 작성하고, 아래 코드로 주석을 .Rd 파일로 전환 및 NAMESPACE에 추가
devtools::document()
# initial recipe 만들어두고 조건에 맞게 한 단계씩 진행하도록
rec <- recipe(Survived ~ ., data = data_train)
rec
str(rec)
data_train_cv
#### define model goophi::trainTestSplit(data, strata)); tibble -> list(train, test)####
lr_mod <-
logistic_reg(penalty = tune(), mixture = 1) %>%
set_engine("glmnet")
lr_mod
str(lr_mod)
?show_best
?select_best
library(tidymodels)  # for the tune package, along with the rest of tidymodels
library(dplyr)
library(recipes)
library(parsnip)
library(tune)
library(rsample)
# install.packages("titanic")
data(titanic_train, package = "titanic")
cleaned_data <- tibble::as_tibble(titanic_train) %>%
select(-PassengerId)
set.seed(1234) # fix seed
#### train-test split (goophi::trainTestSplit(data, strata)); tibble -> list(train, test) ####
## from
# data_split <- rsample::initial_split(cleaned_data, strata = Survived)
#
# data_train <- rsample::training(data_split)
# data_test  <- rsample::testing(data_split)
## to
targetVar <- "Survived"
data_train <- goophi::trainTestSplit(cleaned_data, strata = cleaned_data[[targetVar]])[[1]]
data_test <- goophi::trainTestSplit(cleaned_data, strata = cleaned_data[[targetVar]])[[2]]
nrow(data_train)
nrow(data_train)/nrow(cleaned_data)
data_train %>%
count(Survived) %>%
mutate(prop = n/sum(n))
data_test %>%
count(Survived) %>%
mutate(prop = n/sum(n))
