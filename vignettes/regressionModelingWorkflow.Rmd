---
title: "Regression Modeling Workflow"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{regressionModelingWorkflow}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

## R environment

```{r setup}
library(DT)
library(goophi)
```

#### Camel case : 유저로부터 받는 입력 및 goophi의 함수들 (다만 goophi 함수들 중 algorithm명은 다른 패키지와 겹치는 경우가 많아 '함수명_phi' 로 임시 명명하였습니다. 추후 수정 예정입니다.)
#### Snake case: Shiny app의 server에서 사용(될 것이라고 예상)하는 Object명 및 snake case로 작성된 dependencies의 함수명

## 0. Import sample data

#### 샘플데이터를 불러와 전처리 합니다. (여기까지의 작업은 앞서 이미 진행되었다고 가정)

```{r}
cleaned_data <- readr::read_csv(
    "../data/housePrice.csv") %>%
  dplyr::select(c("SalePrice", "MSSubClass", "LotArea", "OverallQual", "OverallCond", "Street", "Utilities"))
  

rec <- recipes::recipe(SalePrice ~ ., data = cleaned_data) %>%
  recipes::step_naomit(recipes::all_nominal_predictors()) %>%
  recipes::step_dummy(recipes::all_nominal_predictors())

rec_prep <- recipes::prep(rec)

cleaned_data <- recipes::bake(rec_prep, new_data = cleaned_data)
knitr::kable(head(cleaned_data, 10))
```
## 1. Train-test split Tab

#### train set, test set을 분리합니다.

|    User Input |                                    description |
|--------------:|-----------------------------------------------:|
|     targetVar |                       목적 변수(target, label) |
| trainSetRatio | 전체 데이터 중 train set의 비율 (range: 0 - 1) |

### User input

```{r}
targetVar <- "SalePrice"
trainSetRatio <- "0.7"
```

### Train-test split 작업이 완료된 Object를 저장하고, Train set을 보여줍니다.

```{r}
split_tmp <- goophi::trainTestSplit(data = cleaned_data, target = targetVar, prop = trainSetRatio)
data_train <- split_tmp$train # train data
data_test <- split_tmp$test # test data
data_split <- split_tmp$dataSplit # whole data with split information

DT::datatable(data_train, options = list(scrollX = TRUE))
```

## 2. Preprocessing Tab

#### Train set에 대한 전처리 방식을 설정합니다. 예측 변수(features, attributes)는 0번째 단계에서 유저가 원하는 column을 선택했다고 가정하고, 전체를 사용합니다.

|        User Input |                                             description |
|-------------------:|---------------------------------------------------:|
|        imputation |                          imputation 적용 여부 (Boolean) |
|     normalization |                       normalization 적용 여부 (Boolean) |
|    imputationType |                                         imputation 방식 |
| normalizationType |                                      normalization 방식 |

### User input

```{r}
formula <- paste0(targetVar, " ~ .") # user 입력 x (1에서 user가 targetVar를 입력했을 때 함께 생성) 

imputation <- TRUE
normalization <- TRUE
pca <- FALSE
pcaThres <- "0.7"
imputationType = "mean" # 정리필요
normalizationType = "range" # 정리필요
```

### train set에 적용할 전처리 정보를 담은 recipe를 생성합니다

```{r}
rec <- goophi::prepForCV(data = data_train,
                             formula = formula,
                             imputationType = imputationType,
                             normalizationType = normalizationType,
                             imputation = imputation,
                             normalization = normalization)
```

## 3. Modeling with CV Tab

#### grid search, cross validation을 통해 유저가 선택한 모델을 fitting합니다.

| User Input |                                             description |
|-----------:|--------------------------------------------------------:|
|       algo |                                            ML 모델 선택 |
|     engine |                                             engine 선택 |
|       mode |                                               mode 선택 |
|     metric |                   Best performance에 대한 평가지표 선택 |
|          v | Cross validation시 train set을 몇 번 분할할 것인지 입력 |

```{r}
# 모델 object를 저장할 빈 리스트를 생성합니다.
models_list <- list()
```

### User input

```{r}
# 모델링 과정에서 사용할 engine, mode를 입력받습니다. 
mode <- "regression"
algo <- "LightGBM" # 해당 mode에 해당하는 algorithms 보여주기
engine <- "lightgbm" # 해당 algo에 해당하는 engine 보여주기

# algo, engine에 맞는 hyperparameter를 입력받습니다 (모델마다 상이함, 정리필요). 

minNRangeMin <- "20"
minNRangeMax <- "20"
minNRangeLevels <- "10"

mtryRangeMin <- "3"
mtryRangeMax <- "3"
mtryRangeLevels <- "1"

treesRangeMin <- "100"
treesRangeMax <- "500"
treesRangeLevels <- "200"

treeDepthRangeMin <- "2"
treeDepthRangeMax <- "6"
treeDepthRangeLevels <- "2"

learnRateRangeMin <- "0.1"
learnRateRangeMax <- "0.1"
learnRateRangeLevels <- "0.1"

lossReductionRangeMin <- "0"
lossReductionRangeMax <- "0"
lossReductionRangeLevels <- "1"

# training data를 몇 개로 나눌지 입력받습니다.
v <- "2"

# 어떤 지표로 best performance를 평가할건지 입력받습니다(정리필요).
metric <- "rmse"
```

### grid search + cross validation + modeling

```{r}
# 사용자정의 ML 모델을 생성합니다
if (algo == "LightGBM") {
  
  model <- goophi::lightGbm(engine = engine,
                            mode = mode)
  
  ## 이 입력 전부 goophi::lightGbm_phi 안에 넣기
  minNRange <- c(as.numeric(minNRangeMin), as.numeric(minNRangeMax))
  mtryRange <- c(as.numeric(mtryRangeMin), as.numeric(mtryRangeMax))
  treesRange <- c(as.numeric(treesRangeMin), as.numeric(treesRangeMax))
  treeDepthRange <- c(as.numeric(treeDepthRangeMin), as.numeric(treeDepthRangeMax))
  learnRateRange <- c(as.numeric(learnRateRangeMin), as.numeric(learnRateRangeMax))
  lossReductionRange <- c(as.numeric(lossReductionRangeMin), as.numeric(lossReductionRangeMax))
  
  parameterGrid <- dials::grid_regular(
    min_n(range = minNRange),
    mtry(range = mtryRange),
    trees(range = treesRange),
    tree_depth(range = treeDepthRange), # num_leaves = 2^(max_depth)
    learn_rate(range = learnRateRange),
    loss_reduction(range = lossReductionRange),
    levels = c(min_n = as.numeric(minNRangeLevels),
               mtry = as.numeric(mtryRangeLevels),
               trees = as.numeric(treesRangeLevels),
               tree_depth = as.numeric(treeDepthRangeLevels),
               learn_rate = as.numeric(learnRateRangeLevels),
               loss_reduction = as.numeric(lossReductionRangeLevels)
    )
  )
       ## 여기까지
  
  
  # parameter grid를 적용한 cross validation을 수행합니다
  grid_search_result <- goophi::gridSearchCV(rec = rec,
                             model = model,
                             v = v,
                             data = data_train,
                             parameterGrid = parameterGrid
  )
  
  # 가장 성능이 좋은 모델을 저장합니다
  finalized <- goophi::fitBestModel(gridSearchResult = grid_search_result,
                                    metric = metric, # "rmse" "rsq"
                                    model = model,
                                    formula = formula,
                                    trainingData = data_train,
                                    splitedData = data_split,
                                    algo = paste0(algo,"_",engine))
  
  
  models_list <- append(models_list, list(finalized[[2]]))
}
```

### 두 번째 모델을 추가합니다.

### User input

```{r}
mode <- "regression"
algo <- "MLP"
engine <- "nnet"

hiddenUnitsMin <- "1"
hiddenUnitsMax <- "5"
hiddenUnitsLevels <- "2"

penaltyMin <- "0"
penaltyMax <- "1"
penaltyLevels <- "0.5"

epochsMin <- "20"
epochsMax <- "100"
epochsLevels <- "30"

v <- "2"

metric <- "rmse"
```

### grid search + cross validation + modeling

```{r}
if (algo == "MLP") {
  model <- goophi::mlp_phi(engine = engine,
                         mode = mode)

  hiddenUnitsRange <- c(as.numeric(hiddenUnitsMin), as.numeric(hiddenUnitsMax))
  penaltyRange <- c(as.numeric(penaltyMin), as.numeric(penaltyMax))
  epochsRange <- c(as.numeric(epochsMin), as.numeric(epochsMax))
  
  parameterGrid <- dials::grid_regular(
    hidden_units(range = hiddenUnitsRange),
    penalty(range = penaltyRange),
    epochs(range = epochsRange),
    levels = c(hidden_units = as.numeric(hiddenUnitsLevels),
               penalty = as.numeric(penaltyLevels),
               epochs = as.numeric(epochsLevels)
    )
  )
  
  grid_search_result <- goophi::gridSearchCV(rec = rec,
                             model = model,
                             v = v,
                             data = data_train,
                             parameterGrid = parameterGrid
  )
  
  finalized <- goophi::fitBestModel(gridSearchResult = grid_search_result,
                                    metric = metric,
                                    model = model,
                                    formula = formula,
                                    trainingData = data_train,
                                    splitedData = data_split,
                                    algo = paste0(algo,"_",engine))

  models_list <- append(models_list, list(finalized[[2]]))
}

```

### 세 번째 모델을 추가합니다.

### User input

```{r}
mode <- "regression"
algo <- "linearRegression"
engine <- "keras"

penaltyMin <- "0"
penaltyMax <- "1"
penaltyLevels <- "0.5"
mixtureMin <- "0.2"
mixtureMax <- "0.5"
mixtureLevles <- "0.3"

v <- "2"

metric <- "rmse"
```

### grid search + cross validation + modeling

```{r}

model <- goophi::linearRegression_phi(engine = engine,
                       mode = mode)

penaltyRange <- c(as.numeric(penaltyMin), as.numeric(penaltyMax))
mixtureRange <- c(as.numeric(mixtureMin), as.numeric(mixtureMax))


parameterGrid <- dials::grid_regular(
  mixture(range = mixtureRange),
  penalty(range = penaltyRange),
  levels = c(mixture = as.numeric(mixtureLevles),
             penalty = as.numeric(penaltyLevels)
  )
)

grid_search_result <- goophi::gridSearchCV(rec = rec,
                           model = model,
                           v = v,
                           data = data_train,
                           parameterGrid = parameterGrid
)

finalized <- goophi::fitBestModel(gridSearchResult = grid_search_result,
                                  metric = metric,
                                  model = model,
                                  formula = formula,
                                  trainingData = data_train,
                                  splitedData = data_split,
                                  algo = paste0(algo,"_",engine))

models_list <- append(models_list, list(finalized[[2]]))

```
