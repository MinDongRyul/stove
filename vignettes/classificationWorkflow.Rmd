---
title: "Classification Modeling Workflow"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{classificationWorkflow}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

## R environment

```{r setup}
require(dplyr) # import for loading/preprocessing the sample data
require(MASS)
data("Boston")
library(goophi)
```

#### Camel case : 유저로부터 받는 입력 및 goophi의 함수들
#### Snake case: Shiny app의 server에서 사용(될 것이라고 예상)하는 Object명 및 snake case로 작성된 dependencies의 함수명

## 0. Import sample data

#### 샘플데이터를 불러와 전처리 합니다. (여기까지의 작업은 앞서 이미 진행되었다고 가정)

```{r}
cleaned_data <- Boston %>%
  dplyr::mutate(chas = as.factor(chas)) %>%
  dplyr::mutate(Pcrime = ifelse(crim > median(crim), 1, 0)) %>%
  dplyr::mutate(Pcrime = as.factor(Pcrime)) %>%
  dplyr::select(-c(crim))

knitr::kable(head(cleaned_data, 10))
```

## 1. Train-test split Tab

|    User Input |                                    description |
|--------------:|-----------------------------------------------:|
|     targetVar |                       목적 변수(target, label) |
| trainSetRatio | 전체 데이터 중 train set의 비율 (range: 0 - 1) |

### 1) User input

```{r}
targetVar <- "Pcrime"
trainSetRatio <- "0.7"
seed <- "1234"
```

### 2) Train-test split 작업이 완료된 Object를 저장하고, Train set을 보여줍니다.

```{r}
split_tmp <- goophi::trainTestSplit(data = cleaned_data,
                                    target = targetVar,
                                    prop = trainSetRatio,
                                    seed = seed
                                    )

data_train <- split_tmp[[1]] # train data
data_test <- split_tmp[[2]] # test data
data_split <- split_tmp[[3]] # whole data with split information

DT::datatable(data_train, options = list(scrollX = TRUE))
```

## 2. Preprocessing Tab

#### Train set에 대한 전처리 방식을 설정합니다. 예측 변수(features, attributes)는 0. Import sample data 단계에서 유저가 원하는 column을 선택했다고 가정하고, 전체 feature들을 사용합니다.

|        User Input |                                             description |
|-------------------:|---------------------------------------------------:|
|        imputation |                          imputation 적용 여부 (Boolean) |
|     normalization |                       normalization 적용 여부 (Boolean) |
|    imputationType |                                         imputation 방식 |
| normalizationType |                                      normalization 방식 |

### 1) User input

```{r}
formula <- paste0(targetVar, " ~ .") # user 입력 x (1에서 user가 targetVar를 입력했을 때 함께 생성)

imputation <- TRUE
normalization <- TRUE
nominalImputationType <- "mode" # mode(default), bag, knn
numericImputationType <- "mean" # mean(default), bag, knn, linear, lower, median, roll
normalizationType <- "range" # range(default), center, normalization, scale
seed <- "1234"
```

### 2) train set에 적용할 전처리 정보를 담은 recipe를 생성합니다

```{r}
rec <- goophi::prepForCV(data = data_train,
                         formula = formula,
                         imputation = imputation,
                         normalization = normalization,
                         nominalImputationType = nominalImputationType, 
                         numericImputationType = numericImputationType, 
                         normalizationType = normalizationType,
                         seed = seed
                         )
```

## 3. Modeling with CV Tab

#### grid search, cross validation을 통해 유저가 선택한 모델을 fitting합니다.

| User Input |                                             description |
|-----------:|--------------------------------------------------------:|
|       algo |                                            ML 모델 선택 |
|     engine |                                             engine 선택 |
|       mode |                                               mode 선택 |
|     metric |                   Best performance에 대한 평가지표 선택 |
|          v | Cross validation시 train set을 몇 번 분할할 것인지 입력 |

```{r}
# 모델 object를 저장할 빈 리스트를 생성합니다.
models_list <- list()
```


### 1) logistic regression model

```{r, message=FALSE, warning=FALSE}
## User input

mode <- "classification"
algo <- "LogisticR"
engine <- "glmnet" # glm, glmnet, LiblineaR, stan

penaltyRangeMin <- "0.001"
penaltyRangeMax <- "1.0"
penaltyRangeLevels <- "5"
mixtureRangeMin <- "0.0"
mixtureRangeMax <- "1.0"
mixtureRangeLevels <- "5" 

v <- "2"

metric <- "roc_auc"

## grid search + cross validation + modeling

finalized <- goophi::logisticRegression(
  algo = algo,
  engine = engine,
  mode = mode,
  trainingData = data_train,
  splitedData = data_split,
  formula = formula,
  rec = rec,
  v = v,
  penaltyRangeMin = penaltyRangeMin,
  penaltyRangeMax = penaltyRangeMax,
  penaltyRangeLevels = penaltyRangeLevels,
  mixtureRangeMin = mixtureRangeMin,
  mixtureRangeMax = mixtureRangeMax,
  mixtureRangeLevels = mixtureRangeLevels,
  metric = metric
)

## models_list에 모델을 추가합니다.
models_list[[paste0(algo, "_", engine)]] <- finalized$finalFittedModel
```


### 2) K Nearest Neighbor

```{r, message=FALSE, warning=FALSE}
## User input

mode <- "classification"
algo <- "KNN"
engine <- "kknn" # kknn

neighborsRangeMin <- "1" 
neighborsRangeMax <- "10" 
neighborsRangeLevels <- "10" 

v <- "2"

metric <- "roc_auc"

## grid search + cross validation + modeling

finalized <- goophi::KNN(
  algo = algo,
  engine = engine,
  mode = mode,
  trainingData = data_train,
  splitedData = data_split,
  formula = formula,
  rec = rec,
  v = v,
  neighborsRangeMin = neighborsRangeMin,
  neighborsRangeMax = neighborsRangeMax,
  neighborsRangeLevels = neighborsRangeLevels,
  metric = metric
)

## models_list에 모델을 추가합니다.
models_list[[paste0(algo, "_", engine)]] <- finalized$finalFittedModel
```

### 3) Naive Bayes

```{r, message=FALSE, warning=FALSE}
## User input

mode <- "classification"
algo <- "Naive Bayes"
engine <- "klaR" # klaR, naivebayes

smoothnessRangeMin <- "0.5"
smoothnessRangeMax <- "1.5" 
smoothnessRangeLevels <- "3" 
LaplaceRangeMin <- "0.0" 
LaplaceRangeMax <- "3.0"
LaplaceRangeLevels <- "4"

v <- "2"

metric <- "roc_auc"

## grid search + cross validation + modeling

finalized <- goophi::naiveBayes(
  algo = algo,
  engine = engine,
  mode = mode,
  trainingData = data_train,
  splitedData = data_split,
  formula = formula,
  rec = rec,
  v = v,
  smoothnessRangeMin = smoothnessRangeMin,
  smoothnessRangeMax = smoothnessRangeMax,
  smoothnessRangeLevels = smoothnessRangeLevels,
  LaplaceRangeMin = LaplaceRangeMin,
  LaplaceRangeMax = LaplaceRangeMax,
  LaplaceRangeLevels = LaplaceRangeLevels,
  metric = metric
)

## models_list에 모델을 추가합니다.
models_list[[paste0(algo, "_", engine)]] <- finalized$finalFittedModel
```

### 4) Decision Tree

```{r, message=FALSE, warning=FALSE}
## User input

mode <- "classification"
algo <- "Decision Tree"
engine <- "rpart" # rpart, C50, partykit

treeDepthRangeMin <- "1" 
treeDepthRangeMax <- "15" 
treeDepthRangeLevels <- "3" 
minNRangeMin <- "2" 
minNRangeMax <- "40" 
minNRangeLevels <- "3" 
costComplexityRangeMin <- "-2.0"
costComplexityRangeMax <- "-1.0" 
costComplexityRangeLevels <- "2" 

v <- "2"

metric <- "roc_auc"

## grid search + cross validation + modeling

finalized <- goophi::decisionTree(
  algo = algo,
  engine = engine,
  mode = mode,
  trainingData = data_train,
  splitedData = data_split,
  formula = formula,
  rec = rec,
  v = v,
  treeDepthRangeMin = treeDepthRangeMin,
  treeDepthRangeMax = treeDepthRangeMax,
  treeDepthRangeLevels = treeDepthRangeLevels,
  minNRangeMin = minNRangeMin,
  minNRangeMax = minNRangeMax,
  minNRangeLevels = minNRangeLevels,
  costComplexityRangeMin = costComplexityRangeMin,
  costComplexityRangeMax = costComplexityRangeMax,
  costComplexityRangeLevels = costComplexityRangeLevels,
  metric = metric
)

## models_list에 모델을 추가합니다.
models_list[[paste0(algo, "_", engine)]] <- finalized$finalFittedModel
```

### 5) Random Forest

```{r, message=FALSE, warning=FALSE}
## User input

mode <- "classification"
algo <- "Random Forest"
engine <- "ranger" # ranger, randomforest, partykit

mtryRangeMin <- "1" 
mtryRangeMax <- "20" 
mtryRangeLevels <- "3" 
treesRangeMin <- "100" 
treesRangeMax <- "1000" 
treesRangeLevels <- "3" 
minNRangeMin <- "2" 
minNRangeMax <- "40" 
minNRangeLevels <- "3" 

v <- "2"

metric <- "roc_auc"

## grid search + cross validation + modeling

finalized <- goophi::randomForest(
  algo = algo,
  engine = engine,
  mode = mode,
  trainingData = data_train,
  splitedData = data_split,
  formula = formula,
  rec = rec,
  v = v,
  mtryRangeMin = mtryRangeMin,
  mtryRangeMax = mtryRangeMax,
  mtryRangeLevels = mtryRangeLevels,
  treesRangeMin = treesRangeMin,
  treesRangeMax = treesRangeMax,
  treesRangeLevels = treesRangeLevels,
  minNRangeMin = minNRangeMin,
  minNRangeMax = minNRangeMax,
  minNRangeLevels = minNRangeLevels,
  metric = metric
)

## models_list에 모델을 추가합니다.
models_list[[paste0(algo, "_", engine)]] <- finalized$finalFittedModel
```

### 6) XGBoost

```{r, message=FALSE, warning=FALSE}
## User input

mode <- "classification"
algo <- "XGBoost"
engine <- "xgboost" # xgboost

treeDepthRangeMin <- "5" 
treeDepthRangeMax <- "15" 
treeDepthRangeLevels <- "3" 
treesRangeMin <- "8" 
treesRangeMax <- "32" 
treesRangeLevels <- "3" 
learnRateRangeMin <- "-2" 
learnRateRangeMax <- "-1" 
learnRateRangeLevels <- "2" 
mtryRangeMin <- "0.0" 
mtryRangeMax <- "1.0" 
mtryRangeLevels <- "3" 
minNRangeMin <- "2" 
minNRangeMax <- "40" 
minNRangeLevels <- "3" 
lossReductionRangeMin <- "-1.0" 
lossReductionRangeMax <- "1.0" 
lossReductionRangeLevels <- "3" 
sampleSizeRangeMin <- "0.0" 
sampleSizeRangeMax <- "1.0" 
sampleSizeRangeLevels <- "3" 
stopIter <- "30" 

v <- "2"

metric <- "roc_auc"

## grid search + cross validation + modeling

finalized <- goophi::xgBoost(
  algo = algo,
  engine = engine,
  mode = mode,
  trainingData = data_train,
  splitedData = data_split,
  formula = formula,
  rec = rec,
  v = v,
  treeDepthRangeMin = treeDepthRangeMin,
  treeDepthRangeMax = treeDepthRangeMax,
  treeDepthRangeLevels = treeDepthRangeLevels,
  treesRangeMin = treesRangeMin,
  treesRangeMax = treesRangeMax,
  treesRangeLevels = treesRangeLevels,
  learnRateRangeMin = learnRateRangeMin,
  learnRateRangeMax = learnRateRangeMax,
  learnRateRangeLevels = learnRateRangeLevels,
  mtryRangeMin = mtryRangeMin,
  mtryRangeMax = mtryRangeMax,
  mtryRangeLevels = mtryRangeLevels,
  minNRangeMin = minNRangeMin,
  minNRangeMax = minNRangeMax,
  minNRangeLevels = minNRangeLevels,
  lossReductionRangeMin = lossReductionRangeMin,
  lossReductionRangeMax = lossReductionRangeMax,
  lossReductionRangeLevels = lossReductionRangeLevels,
  sampleSizeRangeMin = sampleSizeRangeMin,
  sampleSizeRangeMax = sampleSizeRangeMax,
  sampleSizeRangeLevels = sampleSizeRangeLevels,
  stopIter = stopIter,
  metric = metric
)

## models_list에 모델을 추가합니다.
models_list[[paste0(algo, "_", engine)]] <- finalized$finalFittedModel
```


### 7) lightGBM

```{r, message=FALSE, warning=FALSE}
## User input

mode <- "classification"
algo <- "light GBM"
engine <- "lightgbm" # lightgbm

treeDepthRangeMin <- "5" 
treeDepthRangeMax <- "10" 
treeDepthRangeLevels <- "2" 
treesRangeMin <- "10" 
treesRangeMax <- "500" 
treesRangeLevels <- "3" 
learnRateRangeMin <- "-2" 
learnRateRangeMax <- "-1" 
learnRateRangeLevels <- "2" 
mtryRangeMin <- "1" 
mtryRangeMax <- "20" 
mtryRangeLevels <- "3" 
minNRangeMin <- "2" 
minNRangeMax <- "40" 
minNRangeLevels <- "3" 
lossReductionRangeMin <- "-1" 
lossReductionRangeMax <- "1" 
lossReductionRangeLevels <- "3" 

v <- "2"

metric <- "roc_auc"

## grid search + cross validation + modeling

finalized <- goophi::lightGbm(
  algo = algo,
  engine = engine,
  mode = mode,
  trainingData = data_train,
  splitedData = data_split,
  formula = formula,
  rec = rec,
  v = v,
  treeDepthRangeMin = treeDepthRangeMin,
  treeDepthRangeMax = treeDepthRangeMax,
  treeDepthRangeLevels = treeDepthRangeLevels,
  treesRangeMin = treesRangeMin,
  treesRangeMax = treesRangeMax,
  treesRangeLevels = treesRangeLevels,
  learnRateRangeMin = learnRateRangeMin,
  learnRateRangeMax = learnRateRangeMax,
  learnRateRangeLevels = learnRateRangeLevels,
  mtryRangeMin = mtryRangeMin,
  mtryRangeMax = mtryRangeMax,
  mtryRangeLevels = mtryRangeLevels,
  minNRangeMin = minNRangeMin,
  minNRangeMax = minNRangeMax,
  minNRangeLevels = minNRangeLevels,
  lossReductionRangeMin = lossReductionRangeMin,
  lossReductionRangeMax = lossReductionRangeMax,
  lossReductionRangeLevels = lossReductionRangeLevels,
  metric = metric
)

## models_list에 모델을 추가합니다.
models_list[[paste0(algo, "_", engine)]] <- finalized$finalFittedModel
```
### 8) MLP

```{r, message=FALSE, warning=FALSE}
## User input
# 주석 처리한 hyperparameters는 추후 업데이트 할 engine을 위해 남겨두었습니다.

mode <- "classification"
algo <- "MLP"
engine <- "nnet" # nnet

hiddenUnitsRangeMin <- "1" 
hiddenUnitsRangeMax <- "10" 
hiddenUnitsRangeLevels <- "3" 
penaltyRangeMin <- "0.001" 
penaltyRangeMax <- "1.0" 
penaltyRangeLevels <- "3" 
epochsRangeMin <- "10" 
epochsRangeMax <- "100" 
epochsRangeLevels <- "2" 
# dropoutRangeMin = "0"
# dropoutRangeMax = "1"
# dropoutRangeLevels = "2"
# activation = "relu" #"linear", "softmax", "relu", "elu"
# learnRateRangeMin = "0"
# learnRateRangeMax = "1"
# learnRateRangeLevels = "2"

v <- "2"

metric <- "roc_auc"

## grid search + cross validation + modeling

finalized <- goophi::MLP(
  algo = algo,
  engine = engine,
  mode = mode,
  trainingData = data_train,
  splitedData = data_split,
  formula = formula,
  rec = rec,
  v = v,
  hiddenUnitsRangeMin = hiddenUnitsRangeMin,
  hiddenUnitsRangeMax = hiddenUnitsRangeMax,
  hiddenUnitsRangeLevels = hiddenUnitsRangeLevels,
  penaltyRangeMin = penaltyRangeMin,
  penaltyRangeMax = penaltyRangeMax,
  penaltyRangeLevels = penaltyRangeLevels,
  epochsRangeMin = epochsRangeMin,
  epochsRangeMax = epochsRangeMax,
  epochsRangeLevels = epochsRangeLevels,
  # dropoutRangeMin = dropoutRangeMin,
  # dropoutRangeMax = dropoutRangeMax,
  # dropoutRangeLevels = dropoutRangeLevels,
  # activation = activation, #"linear", "softmax", "relu", "elu"
  # learnRateRangeMin = learnRateRangeMin,
  # learnRateRangeMax = learnRateRangeMax,
  # learnRateRangeLevels = learnRateRangeLevels,
  metric = metric
)

## models_list에 모델을 추가합니다.
models_list[[paste0(algo, "_", engine)]] <- finalized$finalFittedModel
```

## 4. Report Tab

### 1) ROC Curve

#### 유저가 선택한 모델의 ROC curve 출력

```{r, fig.width = 7, fig.height = 7}
roc_curve <- goophi::rocCurve(
  modelsList = models_list,
  targetVar = targetVar
)
roc_curve
```

### 2) Confusion Matrix

#### 유저가 선택한 모델의 confusion matrix 출력

### User input

```{r}
names(models_list)
model_name <- "XGBoost_xgboost" # 원하는 모델의 이름 입력받기
```

```{r, fig.width = 7, fig.height = 6}
cm <- goophi::confusionMatrix(
  modelName = model_name,
  modelsList = models_list,
  targetVar = targetVar
)
cm
```

### 3) Evaluation metrics

#### 모델 성능 평가를 위한 표 출력

```{r}
options(yardstick.event_level = "second") # 오름차순으로 factor의 level 설정된다고 가정
evalMet <- goophi::evalMetricsC(models_list, targetVar)
knitr::kable(evalMet)
```
